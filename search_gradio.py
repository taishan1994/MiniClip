import gradio as gr
import os
import json
import faiss
import numpy as np

from model import MiniClip
from PIL import Image
from transform import image_transform
from tokenizer import tokenize
from tqdm import tqdm
from safetensors.torch import load_file


cfg_path = "model_configs/TinyCLIP-ViT-40M-32-Text-19M.json"
state_dict_path = "/data/gongoubo/MiniClip/checkpoints/model.safetensors"
clip = MiniClip(cfg_path)

state_dict = load_file(state_dict_path)
for k,v in state_dict.items():
    print(k, v.shape)
clip.load_state_dict(state_dict, strict=True)

image_features = np.load("output/image2.npy").astype('float32')
d = image_features.shape[1]
index = faiss.IndexFlatL2(d)
index.add(image_features)
with open("data/en_val.json", "r") as fp:
    data = json.loads(fp.read())
image_paths = {i:os.path.join("/data/gongoubo/MiniClip/data", d["image"].replace("\\", "/")) for i,d in enumerate(data)}

# å¤„ç†æ–‡æœ¬ query -> ç‰¹å¾å‘é‡
def encode_text(query):
    text_input = tokenize(query)
    text_features = clip.encode_text(text_input, normalized=True)
    text_features = text_features.detach().cpu().numpy().astype('float32')
    return text_features


# æ£€ç´¢å‡½æ•°
def search_images(query, top_k=20):
    text_vector = encode_text(query)  # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é… FAISS
    print(text_vector.shape)
    _, indices = index.search(text_vector, top_k)  # æ£€ç´¢ top_k ä¸ªæœ€ç›¸ä¼¼å›¾ç‰‡
    retrieved_images = [Image.open(image_paths[i]) for i in indices[0]]  # åŠ è½½å›¾ç‰‡
    return retrieved_images


# Gradio ç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ” æ–‡æœ¬æ£€ç´¢å›¾ç‰‡")
    with gr.Row():
        query_input = gr.Textbox(label="è¾“å…¥æŸ¥è¯¢æ–‡æœ¬")
        search_button = gr.Button("æœç´¢")

    gallery = gr.Gallery(label="æ£€ç´¢ç»“æœ", columns=[10], height=300)  # ä»¥ç½‘æ ¼å±•ç¤ºå›¾ç‰‡

    search_button.click(fn=search_images, inputs=[query_input], outputs=[gallery])

# è¿è¡Œ Gradio
demo.launch(server_name="0.0.0.0", server_port=7860)
